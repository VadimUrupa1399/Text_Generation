{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-13T16:48:06.376165Z","iopub.execute_input":"2022-05-13T16:48:06.376542Z","iopub.status.idle":"2022-05-13T16:48:06.415561Z","shell.execute_reply.started":"2022-05-13T16:48:06.376445Z","shell.execute_reply":"2022-05-13T16:48:06.414303Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \n\n# set seeds for reproducability\nfrom tensorflow.random import set_seed\nfrom numpy.random import seed\nset_seed(2)\nseed(1)\n\nimport pandas as pd\nimport numpy as np\nimport string, os \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T16:48:06.417365Z","iopub.execute_input":"2022-05-13T16:48:06.417898Z","iopub.status.idle":"2022-05-13T16:48:12.361377Z","shell.execute_reply.started":"2022-05-13T16:48:06.417848Z","shell.execute_reply":"2022-05-13T16:48:12.360531Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import glob","metadata":{"execution":{"iopub.status.busy":"2022-05-13T16:48:24.842224Z","iopub.execute_input":"2022-05-13T16:48:24.842516Z","iopub.status.idle":"2022-05-13T16:48:24.846979Z","shell.execute_reply.started":"2022-05-13T16:48:24.842476Z","shell.execute_reply":"2022-05-13T16:48:24.845657Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"folder = os.path.dirname('../input/text-generator/')\nfolder\ncsv_files = glob.glob(os.path.join(folder, \"*.csv\"))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T16:52:07.066694Z","iopub.execute_input":"2022-05-13T16:52:07.067126Z","iopub.status.idle":"2022-05-13T16:52:07.072381Z","shell.execute_reply.started":"2022-05-13T16:52:07.067094Z","shell.execute_reply":"2022-05-13T16:52:07.071590Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for f in csv_files:\n      \n    # read the csv file\n    df = pd.read_csv(f)\n      \n    # print the location and filename\n   # print('Location:', f)\n  #  print('File Name:', f.split(\"\\\\\")[-1])\n   #   \n    # print the content\n    print('Content:')\n    display(df)\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T16:52:09.026366Z","iopub.execute_input":"2022-05-13T16:52:09.026754Z","iopub.status.idle":"2022-05-13T16:52:52.584659Z","shell.execute_reply.started":"2022-05-13T16:52:09.026725Z","shell.execute_reply":"2022-05-13T16:52:52.583823Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"curr_dir = '../input/text-generator/'\nall_headlines = []\nfor filename in os.listdir(curr_dir):\n    if 'Articles' in filename:\n        article_df = pd.read_csv(curr_dir + filename)\n        all_headlines.extend(list(article_df.headline.values))\n        break\n\nall_headlines = [h for h in all_headlines if h != \"Unknown\"]\nlen(all_headlines)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T16:54:03.192962Z","iopub.execute_input":"2022-05-13T16:54:03.193247Z","iopub.status.idle":"2022-05-13T16:54:03.215177Z","shell.execute_reply.started":"2022-05-13T16:54:03.193215Z","shell.execute_reply":"2022-05-13T16:54:03.214288Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Removing punctuation and lowercase all words**","metadata":{}},{"cell_type":"code","source":"def clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \n\ncorpus = [clean_text(x) for x in all_headlines]\ncorpus[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-13T16:54:06.538649Z","iopub.execute_input":"2022-05-13T16:54:06.538951Z","iopub.status.idle":"2022-05-13T16:54:06.554307Z","shell.execute_reply.started":"2022-05-13T16:54:06.538917Z","shell.execute_reply":"2022-05-13T16:54:06.553352Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**The next step is to generate sequences of N-gram tokens. The machine learning model of generating text requires a sequence of input data, because, given a sequence (of words/tokens), the goal is to predict the next word/token. For this task, we need to do some tokenization on the dataset**\n\n**Tokenization is a process of extracting tokens from a corpus. Pythonâ€™s Keras library has a built-in tokenization model that can be used to get tokens and their index in the corpus. After this step, each text document in the dataset is converted into a sequence of tokens**","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\n\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    \n    ## convert data to sequence of tokens \n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_sequence_of_tokens(corpus)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T16:58:40.074299Z","iopub.execute_input":"2022-05-13T16:58:40.075741Z","iopub.status.idle":"2022-05-13T16:58:40.123744Z","shell.execute_reply.started":"2022-05-13T16:58:40.075682Z","shell.execute_reply":"2022-05-13T16:58:40.122756Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**Padding the Sequences**\n> Now that we have generated a dataset that contains the sequence of tokens, but be aware that different sequences can have different lengths. So, before we start training the text generation model, we need to fill in the sequences and make their lengths equal","metadata":{}},{"cell_type":"code","source":"from keras.utils.np_utils import to_categorical","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:01:21.042075Z","iopub.execute_input":"2022-05-13T17:01:21.042401Z","iopub.status.idle":"2022-05-13T17:01:21.047242Z","shell.execute_reply.started":"2022-05-13T17:01:21.042366Z","shell.execute_reply":"2022-05-13T17:01:21.046264Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:01:54.346782Z","iopub.execute_input":"2022-05-13T17:01:54.347592Z","iopub.status.idle":"2022-05-13T17:01:54.388140Z","shell.execute_reply.started":"2022-05-13T17:01:54.347547Z","shell.execute_reply":"2022-05-13T17:01:54.387309Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**Using LSTM for Text Generation with Python**","metadata":{}},{"cell_type":"code","source":"def create_model(max_sequence_len, total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n    \n    # Add Input Embedding Layer\n    model.add(Embedding(total_words, 10, input_length=input_len))\n    \n    # Add Hidden Layer 1 - LSTM Layer\n    model.add(LSTM(100))\n    model.add(Dropout(0.1))\n    \n    # Add Output Layer\n    model.add(Dense(total_words, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model\n\nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:06:08.703762Z","iopub.execute_input":"2022-05-13T17:06:08.704288Z","iopub.status.idle":"2022-05-13T17:06:09.236167Z","shell.execute_reply.started":"2022-05-13T17:06:08.704247Z","shell.execute_reply":"2022-05-13T17:06:09.235336Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.fit(predictors, label, epochs=100, verbose=5)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:06:46.143414Z","iopub.execute_input":"2022-05-13T17:06:46.143721Z","iopub.status.idle":"2022-05-13T17:11:01.038473Z","shell.execute_reply.started":"2022-05-13T17:06:46.143690Z","shell.execute_reply":"2022-05-13T17:11:01.037463Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"**Testing**","metadata":{}},{"cell_type":"code","source":"def generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        #predicted = model.predict_classes(token_list, verbose=0)\n        #predicted = (model.predict(token_list) > 0.5).astype(\"int32\")\n        predicted = np.argmax(model.predict(token_list), axis=-1)\n        \n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title()\n   \nprint (generate_text(\"united states\", 10, model, max_sequence_len))\nprint (generate_text(\"preident trump\", 10, model, max_sequence_len))\nprint (generate_text(\"donald trump\", 10, model, max_sequence_len))\nprint (generate_text(\"india and china\", 10, model, max_sequence_len))\nprint (generate_text(\"new york\", 10, model, max_sequence_len))\nprint (generate_text(\"science and technology\", 10, model, max_sequence_len))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T17:18:49.042139Z","iopub.execute_input":"2022-05-13T17:18:49.042456Z","iopub.status.idle":"2022-05-13T17:18:52.546279Z","shell.execute_reply.started":"2022-05-13T17:18:49.042424Z","shell.execute_reply":"2022-05-13T17:18:52.544463Z"},"trusted":true},"execution_count":35,"outputs":[]}]}